"""
üìÇ CCTV Ïù¥ÏÉÅÌñâÎèô Îç∞Ïù¥ÌÑ∞ÏÖã (Ï†ÑÏ≤òÎ¶¨Îêú ÌîÑÎ†àÏûÑ Í∏∞Î∞ò)
- make_processed_frames.py Î°ú ÏÉùÏÑ±Îêú processed_frames Î•º ÏùΩÏñ¥Ïò¥
"""

import cv2
import torch
import numpy as np
from torch.utils.data import Dataset
from pathlib import Path
from torchvision import transforms


class CCTVDataset(Dataset):
    """CCTV Ïù¥ÏÉÅÌñâÎèô Îç∞Ïù¥ÌÑ∞ÏÖã (processed_frames Í∏∞Î∞ò)"""

    def __init__(self, root_dir, split="train", num_frames=16, img_size=224):
        """
        Args:
            root_dir: datasets/ Í≤ΩÎ°ú
            split: 'train' or 'val'
            num_frames: Ï∂îÏ∂úÌï† ÌîÑÎ†àÏûÑ Ïàò
            img_size: ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ ÌÅ¨Í∏∞
        """
        self.root_dir = Path(root_dir)
        self.split = split
        self.num_frames = num_frames
        self.img_size = img_size

        # ÌÅ¥ÎûòÏä§ Ï†ïÏùò (ÏòÅÎ¨∏Î™Ö Í∏∞Ï§Ä)
        self.classes = ["crowd", "fight", "fall"]
        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}

        # processed_frames Í≤ΩÎ°ú
        self.frame_root = self.root_dir / "processed_frames" / self.split
        self.videos = self._load_preprocessed()

        print(f"‚úÖ {split.upper()}: {len(self.videos)}Í∞ú ÏòÅÏÉÅ (Ï†ÑÏ≤òÎ¶¨ ÌîÑÎ†àÏûÑ Í∏∞Î∞ò)")

        # Î≥ÄÌôò Ï†ïÏùò
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Resize((img_size, img_size)),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225]),
        ])

    def _load_preprocessed(self):
        """processed_frames/{split}/{class}/ ÎÇ¥ Î™®Îì† ÏòÅÏÉÅ Ìè¥Îçî Î°úÎìú"""
        videos = []
        for class_name in self.classes:
            class_dir = self.frame_root / class_name
            if not class_dir.exists():
                print(f"‚ö†Ô∏è Missing class dir: {class_dir}")
                continue

            for video_dir in sorted(class_dir.iterdir()):
                if video_dir.is_dir():
                    videos.append({
                        "class": class_name,
                        "video": video_dir.name
                    })
        return videos

    def _sample_frames(self, frame_dir):
        """ÌîÑÎ†àÏûÑ Ìè¥ÎçîÏóêÏÑú num_framesÎßåÌÅº Í∑†Îì± ÏÉòÌîåÎßÅ"""
        frames = sorted(frame_dir.glob("*.jpg"))
        total = len(frames)
        if total == 0:
            raise ValueError(f"‚ùå No frames found in {frame_dir}")

        if total >= self.num_frames:
            indices = np.linspace(0, total - 1, self.num_frames, dtype=int)
            selected = [frames[i] for i in indices]
        else:
            # Î∂ÄÏ°±ÌïòÎ©¥ Î∞òÎ≥µÌï¥ÏÑú Ï±ÑÏõÄ
            repeat_factor = int(np.ceil(self.num_frames / total))
            selected = (frames * repeat_factor)[:self.num_frames]

        return selected

    def __getitem__(self, idx):
        info = self.videos[idx]
        class_name = info["class"]
        label = self.class_to_idx[class_name]
        frame_dir = self.frame_root / class_name / info["video"]

        frame_paths = self._sample_frames(frame_dir)
        frames = []

        for p in frame_paths:
            img = cv2.imread(str(p))
            if img is None:
                continue
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = self.transform(img)
            frames.append(img)

        if len(frames) == 0:
            raise ValueError(f"‚ùå No valid frames in {frame_dir}")

        frames = torch.stack(frames)
        return frames, label

    def __len__(self):
        return len(self.videos)
